% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/generator-architectures.R
\name{SelfAttention}
\alias{SelfAttention}
\title{Self-Attention Layer for Tabular Data}
\usage{
SelfAttention(embed_dim, num_heads = 4, dropout = 0.1)
}
\arguments{
\item{embed_dim}{The dimension of the input embeddings (hidden layer size)}

\item{num_heads}{Number of attention heads. Must divide embed_dim evenly. Defaults to 4.}

\item{dropout}{Dropout rate for attention weights. Defaults to 0.1.}
}
\value{
A torch::nn_module for self-attention
}
\description{
Multi-head self-attention layer that captures relationships between
features in the hidden representation. This allows the generator to learn
dependencies between different columns (e.g., age correlates with income).
}
\keyword{internal}
