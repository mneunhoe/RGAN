% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/generator-architectures.R
\name{TabularGenerator}
\alias{TabularGenerator}
\title{Tabular Generator with Gumbel-Softmax}
\usage{
TabularGenerator(
  noise_dim,
  output_info,
  hidden_units = list(256, 256),
  dropout_rate = 0,
  tau = 0.2,
  normalization = "batch",
  activation = "relu",
  init_method = "xavier_uniform",
  residual = TRUE,
  attention = FALSE,
  attention_heads = 4,
  attention_dropout = 0.1
)
}
\arguments{
\item{noise_dim}{The length of the noise vector per example}

\item{output_info}{A list describing the output structure from data_transformer$output_info.
Each element is a list with (dimension, type) where type is "linear", "mode_specific", or "softmax".}

\item{hidden_units}{A list of the number of neurons per layer. Defaults to list(256, 256)
as used in CTGAN.}

\item{dropout_rate}{The dropout rate for each hidden layer. Only used when
normalization is "none". Defaults to 0.0.}

\item{tau}{Temperature for Gumbel-Softmax. Lower values produce more discrete outputs.
Defaults to 0.2.}

\item{normalization}{Type of normalization to use: "batch" (default, as in CTGAN),
"layer", or "none". Batch normalization is generally preferred for GANs.}

\item{activation}{Activation function: "relu" (default, as in CTGAN), "leaky_relu",
"gelu", or "silu". GELU and SiLU are modern alternatives that can improve performance.}

\item{init_method}{Weight initialization method: "xavier_uniform" (default),
"xavier_normal", "kaiming_uniform", or "kaiming_normal". Xavier is generally
preferred for networks with tanh/sigmoid outputs.}

\item{residual}{Enable residual connections between layers of the same width.
Defaults to TRUE.}

\item{attention}{Enable self-attention layers after residual blocks. Can be TRUE (add
attention after each block), FALSE (no attention), or a vector of layer indices
where attention should be added (e.g., c(2, 4) adds attention after blocks 2 and 4).
Defaults to FALSE.}

\item{attention_heads}{Number of attention heads. Must divide hidden layer size evenly.
Defaults to 4.}

\item{attention_dropout}{Dropout rate for attention weights. Defaults to 0.1.}

\item{n}{Number of blocks to activate (1 to num_blocks)
Get current number of active blocks}
}
\value{
A torch::nn_module for the Tabular Generator
}
\description{
Provides a torch::nn_module Generator for tabular data that applies
Gumbel-Softmax to categorical outputs for differentiable sampling. This improves
gradient flow for discrete variables compared to standard softmax.

Supports state-of-the-art architectural choices from CTGAN and other modern
tabular GAN architectures:
\itemize{
\item \strong{Residual connections:} Skip connections that improve gradient flow
in deeper networks (enabled by default when consecutive layers have same width)
\item \strong{Batch Normalization:} Stabilizes training (CTGAN default)
\item \strong{Layer Normalization:} Alternative that works better with small batches
\item \strong{Multiple activation functions:} ReLU, LeakyReLU, GELU, SiLU
\item \strong{Weight initialization:} Xavier or Kaiming initialization
\item \strong{Self-Attention:} Captures relationships between features
\item \strong{Progressive Training:} Gradually increase network capacity
}
}
\examples{
\dontrun{
# Basic usage with CTGAN-style defaults
output_info <- list(list(1, "linear"), list(3, "softmax"))
gen <- TabularGenerator(noise_dim = 128, output_info = output_info)

# With self-attention for capturing feature relationships
gen <- TabularGenerator(
  noise_dim = 128,
  output_info = output_info,
  hidden_units = list(256, 256, 256),
  attention = TRUE,
  attention_heads = 8
)

# Custom architecture with layer normalization and GELU
gen <- TabularGenerator(
  noise_dim = 128,
  output_info = output_info,
  hidden_units = list(256, 256, 256),
  normalization = "layer",
  activation = "gelu",
  init_method = "kaiming_uniform"
)
}
Set number of active blocks for progressive training
}
