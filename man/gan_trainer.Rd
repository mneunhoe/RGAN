% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gan-trainer.R
\name{gan_trainer}
\alias{gan_trainer}
\title{gan_trainer}
\usage{
gan_trainer(
  data,
  noise_dim = 2,
  noise_distribution = "normal",
  value_function = "original",
  gp_lambda = 10,
  data_type = "tabular",
  generator = NULL,
  generator_optimizer = NULL,
  discriminator = NULL,
  discriminator_optimizer = NULL,
  base_lr = 1e-04,
  ttur_factor = 4,
  weight_clipper = NULL,
  batch_size = 50,
  epochs = 150,
  plot_progress = FALSE,
  plot_interval = "epoch",
  eval_dropout = FALSE,
  synthetic_examples = 500,
  plot_dimensions = c(1, 2),
  track_loss = FALSE,
  plot_loss = FALSE,
  device = "cpu",
  seed = NULL,
  validation_data = NULL,
  early_stopping = FALSE,
  patience = 10,
  lr_schedule = "constant",
  lr_decay_factor = 0.1,
  lr_decay_steps = 50,
  pac = 1,
  output_info = NULL,
  gumbel_tau = 0.2,
  generator_hidden_units = list(256, 256),
  generator_normalization = "batch",
  generator_activation = "relu",
  generator_init = "xavier_uniform",
  generator_residual = TRUE,
  checkpoint_epochs = NULL,
  checkpoint_path = NULL
)
}
\arguments{
\item{data}{Input a data set. Needs to be a matrix, array, torch::torch_tensor or torch::dataset.}

\item{noise_dim}{The dimensions of the GAN noise vector z. Defaults to 2.}

\item{noise_distribution}{The noise distribution. Expects a function that samples from a distribution and returns a torch_tensor. For convenience "normal" and "uniform" will automatically set a function. Defaults to "normal".}

\item{value_function}{The value function for GAN training. Expects a function that takes discriminator scores of real and fake data as input and returns a list with the discriminator loss and generator loss. For convenience four loss functions "original", "wasserstein", "wgan-gp", and "f-wgan" are already implemented. Defaults to "original".}

\item{gp_lambda}{The gradient penalty coefficient for WGAN-GP training. Only used when value_function is "wgan-gp". Defaults to 10.}

\item{data_type}{"tabular" or "image", controls the data type, defaults to "tabular".}

\item{generator}{The generator network. Expects a neural network provided as torch::nn_module. Default is NULL which will create a simple fully connected neural network.}

\item{generator_optimizer}{The optimizer for the generator network. Expects a torch::optim_xxx function, e.g. torch::optim_adam(). Default is NULL which will setup \code{torch::optim_adam(g_net$parameters, lr = base_lr)}.}

\item{discriminator}{The discriminator network. Expects a neural network provided as torch::nn_module. Default is NULL which will create a simple fully connected neural network.}

\item{discriminator_optimizer}{The optimizer for the generator network. Expects a torch::optim_xxx function, e.g. torch::optim_adam(). Default is NULL which will setup \code{torch::optim_adam(g_net$parameters, lr = base_lr * ttur_factor)}.}

\item{base_lr}{The base learning rate for the optimizers. Default is 0.0001. Only used if no optimizer is explicitly passed to the trainer.}

\item{ttur_factor}{A multiplier for the learning rate of the discriminator, to implement the two time scale update rule.}

\item{weight_clipper}{The wasserstein GAN puts some constraints on the weights of the discriminator, therefore weights are clipped during training.}

\item{batch_size}{The number of training samples selected into the mini batch for training. Defaults to 50.}

\item{epochs}{The number of training epochs. Defaults to 150.}

\item{plot_progress}{Monitor training progress with plots. Defaults to FALSE.}

\item{plot_interval}{Number of training steps between plots. Input number of steps or "epoch". Defaults to "epoch".}

\item{eval_dropout}{Should dropout be applied during the sampling of synthetic data? Defaults to FALSE.}

\item{synthetic_examples}{Number of synthetic examples that should be generated. Defaults to 500. For image data e.g. 16 would be more reasonable.}

\item{plot_dimensions}{If you monitor training progress with a plot which dimensions of the data do you want to look at? Defaults to c(1, 2), i.e. the first two columns of the tabular data.}

\item{track_loss}{Store the training losses as additional output. Defaults to FALSE.}

\item{plot_loss}{Monitor the losses during training with plots. Defaults to FALSE.}

\item{device}{Input on which device (e.g. "cpu", "cuda", or "mps") training should be done. Defaults to "cpu".}

\item{seed}{Optional seed for reproducibility. Sets both R's random seed and torch's random seed. Defaults to NULL (no seed).}

\item{validation_data}{Optional validation data for monitoring training. Should be in the same format as training data.}

\item{early_stopping}{Enable early stopping based on validation metrics. Defaults to FALSE.}

\item{patience}{Number of epochs without improvement before stopping. Only used if early_stopping is TRUE. Defaults to 10.}

\item{lr_schedule}{Learning rate schedule type. One of "constant" (default), "step", "exponential", or "cosine".
"step" reduces LR by lr_decay_factor every lr_decay_steps epochs.
"exponential" applies lr_decay_factor decay each epoch.
"cosine" uses cosine annealing from base_lr to 0 over all epochs.}

\item{lr_decay_factor}{Multiplicative factor for learning rate decay. Used with "step" and "exponential" schedules. Defaults to 0.1.}

\item{lr_decay_steps}{Number of epochs between learning rate reductions for "step" schedule. Defaults to 50.}

\item{pac}{Number of samples to pack together for PacGAN (reduces mode collapse). The discriminator
sees \code{pac} samples concatenated together, helping it detect lack of diversity. Must divide batch_size
evenly. Defaults to 1 (standard GAN, no packing). Common values are 8 or 10.}

\item{output_info}{Optional output structure from data_transformer$output_info. When provided,
enables Gumbel-Softmax for categorical columns, improving gradient flow for discrete variables.
Each element should be a list with (dimension, type) where type is "linear", "mode_specific", or "softmax".}

\item{gumbel_tau}{Temperature for Gumbel-Softmax. Lower values (e.g., 0.2) produce more discrete
outputs. Only used when output_info is provided. Defaults to 0.2.}

\item{generator_hidden_units}{List of hidden layer sizes for TabularGenerator. Defaults to
list(256, 256) as used in CTGAN. Only used when output_info is provided.}

\item{generator_normalization}{Normalization type for TabularGenerator: "batch" (default, CTGAN-style),
"layer", or "none". Only used when output_info is provided.}

\item{generator_activation}{Activation function for TabularGenerator: "relu" (default), "leaky_relu",
"gelu", or "silu". Only used when output_info is provided.}

\item{generator_init}{Weight initialization for TabularGenerator: "xavier_uniform" (default),
"xavier_normal", "kaiming_uniform", or "kaiming_normal". Only used when output_info is provided.}

\item{generator_residual}{Enable residual connections in TabularGenerator. Defaults to TRUE.
Only used when output_info is provided.}

\item{checkpoint_epochs}{Interval for saving model checkpoints (in epochs). If NULL (default),
no checkpoints are saved. For example, checkpoint_epochs = 10 saves checkpoints at epochs
10, 20, 30, etc. Checkpoints enable post-GAN boosting for improved sample quality.}

\item{checkpoint_path}{Optional path for disk-based checkpoint persistence. If NULL (default),
checkpoints are stored in memory only. If provided, checkpoints are saved to disk, enabling
post-GAN boosting for large training runs with many checkpoints.}
}
\value{
gan_trainer trains the neural networks and returns an object of class trained_RGAN that contains the last generator, discriminator and the respective optimizers, as well as the settings.
}
\description{
Provides a function to quickly train a GAN model.
}
\examples{
\dontrun{
# Before running the first time the torch backend needs to be installed
torch::install_torch()
# Load data
data <- sample_toydata()
# Build new transformer
transformer <- data_transformer$new()
# Fit transformer to data
transformer$fit(data)
# Transform data and store as new object
transformed_data <-  transformer$transform(data)
# Train the default GAN
trained_gan <- gan_trainer(transformed_data)
# Sample synthetic data from the trained GAN
synthetic_data <- sample_synthetic_data(trained_gan, transformer)
# Plot the results
GAN_update_plot(data = data,
synth_data = synthetic_data,
main = "Real and Synthetic Data after Training")
}
}
