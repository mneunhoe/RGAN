---
title: "Training a State-of-the-Art GAN on the Adult Dataset"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Training a State-of-the-Art GAN on the Adult Dataset}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

This vignette demonstrates how to train a state-of-the-art Generative Adversarial Network
(GAN) for tabular data synthesis using RGAN. We use the UCI Adult dataset as an example,
showcasing all the modern techniques available in the package:

- **Mode-specific normalization** for handling multi-modal continuous distributions
- **Gumbel-Softmax** for differentiable categorical sampling
- **Self-attention layers** for capturing feature relationships
- **Residual connections** for better gradient flow
- **WGAN-GP** for stable training
- **PacGAN** for reducing mode collapse
- **Learning rate scheduling** for improved convergence
- **Early stopping** with validation data

## Setup

```{r setup}
library(RGAN)
library(torch)

# Set seed for reproducibility
set.seed(42)

# Check for GPU availability
device <- if (cuda_is_available()) {
  "cuda"
} else if (torch::backends_mps_is_available()) {
 "mps"
} else {
  "cpu"
}

device <- "cpu"
cat("Using device:", device, "\n")
```

## Loading and Preparing the Adult Dataset

The Adult dataset (also known as the Census Income dataset) contains demographic
information used to predict whether income exceeds $50K/year. It has a mix of
continuous and categorical variables, making it an ideal benchmark for tabular GANs.

```{r load-data}
# Download the Adult dataset from UCI Machine Learning Repository
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"

# Column names (from adult.names file)
col_names <- c(
  "age", "workclass", "fnlwgt", "education", "education_num",
  "marital_status", "occupation", "relationship", "race", "sex",
  "capital_gain", "capital_loss", "hours_per_week", "native_country", "income"
)

# Read the data
adult <- read.csv(
  url,
  header = FALSE,
  col.names = col_names,
  na.strings = " ?",
  strip.white = TRUE,
  stringsAsFactors = FALSE
)

# Check dimensions
cat("Dataset size:", nrow(adult), "rows,", ncol(adult), "columns\n")

# Remove rows with missing values for cleaner training
adult <- na.omit(adult)
cat("After removing NAs:", nrow(adult), "rows\n")

# Select columns for synthesis (dropping fnlwgt and native_country for simplicity)
adult <- adult[, c(
  "age", "workclass", "education", "education_num", "marital_status",
  "occupation", "relationship", "race", "sex",
  "capital_gain", "capital_loss", "hours_per_week", "income"
)]

# View the data structure
head(adult)
str(adult)

# Summary statistics for continuous variables
cat("\n=== Continuous Variables Summary ===\n")
summary(adult[, c("age", "education_num", "capital_gain", "capital_loss", "hours_per_week")])
```

## Data Transformation with Mode-Specific Normalization

For tabular data with mixed types, proper preprocessing is crucial. RGAN's
`data_transformer` handles this automatically:

- **Continuous columns**: Mode-specific normalization using Gaussian Mixture Models (GMM)
  captures multi-modal distributions (e.g., capital_gain has a spike at 0)
- **Categorical columns**: One-hot encoding with Gumbel-Softmax for differentiable sampling

```{r transform-data}
# Create and fit the data transformer
transformer <- data_transformer$new()

# Define which columns are categorical
discrete_cols <- c(
  "workclass", "education", "marital_status",
  "occupation", "relationship", "race", "sex", "income"
)

# Fit with mode-specific normalization for continuous columns
# n_modes controls the number of GMM components (more modes = more expressive)
# Use more modes (15-20) for columns with complex distributions like capital_gain
transformer$fit(
  adult,
  discrete_columns = discrete_cols,
  mode_specific = TRUE,  # Enable mode-specific normalization
  n_modes = 15           # More GMM components for complex distributions
)

cat("Continuous columns:", setdiff(names(adult), discrete_cols), "\n")
cat("Categorical columns:", discrete_cols, "\n")

# Transform the data
transformed_data <- transformer$transform(adult)

cat("Original data shape:", dim(adult), "\n")
cat("Transformed data shape:", dim(transformed_data), "\n")
cat("Output info structure:\n")
print(transformer$output_info)
```

The `output_info` structure tells the generator how to process each output block:
- `"linear"`: Continuous values with tanh activation
- `"mode_specific"`: Mode probabilities (softmax) + normalized value (tanh)
- `"softmax"`: Categorical one-hot encoding with Gumbel-Softmax

## Splitting Data for Validation

Early stopping requires validation data to monitor training progress:

```{r split-data}
# Split into training and validation sets (90/10)
n_train <- floor(0.9 * nrow(transformed_data))
train_idx <- sample(1:nrow(transformed_data), n_train)

train_data <- transformed_data[train_idx, ]
val_data <- transformed_data[-train_idx, ]

cat("Training samples:", nrow(train_data), "\n")
cat("Validation samples:", nrow(val_data), "\n")
```

## Training Configuration

Now we configure the GAN with state-of-the-art settings:

```{r train-gan}
# Train the GAN with all SOTA features
trained_gan <- gan_trainer(
  data = train_data,

  # === Noise Configuration ===
  noise_dim = 128,                    # Latent dimension (128 is common for tabular)
  noise_distribution = "normal",      # Standard normal noise

  # === Value Function ===
  value_function = "wgan-gp",         # Wasserstein GAN with gradient penalty
  gp_lambda = 10,                     # Gradient penalty coefficient

  # === Generator Architecture (TabularGenerator) ===
  output_info = transformer$output_info,  # Enable Gumbel-Softmax
  gumbel_tau = 0.1,                   # Lower temperature = more discrete categoricals
  generator_hidden_units = list(256, 256, 256, 256),  # 4 hidden layers for more capacity
  generator_normalization = "batch",   # Batch normalization (CTGAN default)
  generator_activation = "leaky_relu", # LeakyReLU often works better than ReLU
  generator_init = "xavier_uniform",   # Xavier initialization
  generator_residual = TRUE,           # Enable residual connections

  # === Training Parameters ===
  batch_size = 500,                   # Batch size (larger = more stable gradients)
  epochs = 500,                       # More epochs for better convergence
  base_lr = 1e-4,                     # Slightly lower LR for stability
  ttur_factor = 2,                    # Train discriminator 2x faster (TTUR)

  # === PacGAN for Mode Collapse Prevention ===
  pac = 10,                           # Pack 10 samples together

  # === Learning Rate Schedule ===
  lr_schedule = "cosine",             # Cosine annealing

  # === Early Stopping ===
  validation_data = val_data,
  early_stopping = TRUE,
  patience = 50,                      # More patience - GANs can recover

  # === Monitoring ===
  track_loss = TRUE,                  # Track losses for plotting
  plot_progress = FALSE,              # Set TRUE for interactive monitoring

  # === Device and Reproducibility ===
  device = device,
  seed = 42
)

# Print training summary
print(trained_gan)
```

### Understanding the Configuration

**WGAN-GP**: Uses the Wasserstein distance with gradient penalty instead of the
original GAN loss. This provides more stable training and meaningful loss values.

**Gumbel-Softmax**: Enables gradient flow through categorical variables by using
a continuous relaxation of the categorical distribution. The temperature parameter
`gumbel_tau` controls how "soft" the samples are (lower = more discrete).

**Residual Connections**: Skip connections between layers of the same width improve
gradient flow in deeper networks.

**PacGAN**: The discriminator sees packs of `pac` samples concatenated together,
helping it detect lack of diversity and reducing mode collapse.

**Cosine Annealing**: Learning rate decreases following a cosine curve from
`base_lr` to 0 over the training epochs.

## Monitoring Training Progress

If you enabled `track_loss = TRUE`, you can visualize the training:

```{r plot-losses}
# Plot training losses (requires track_loss = TRUE during training)
if (!is.null(trained_gan$losses)) {
  plot_losses(trained_gan, smooth = 0.9)
}
```

## Generating Synthetic Data

Once trained, generate synthetic samples:

```{r generate-synthetic}
# Generate synthetic data
n_synthetic <- 1000
synthetic_data <- sample_synthetic_data(
  trained_gan,
  transformer,
  n = n_synthetic
)

cat("Synthetic data shape:", dim(synthetic_data), "\n")
head(synthetic_data)
```

## Evaluating Synthetic Data Quality

### Visual Comparison

```{r visual-comparison, fig.width=10, fig.height=8}
# Compare distributions of continuous variables
par(mfrow = c(2, 3))

continuous_cols <- c("age", "education_num", "capital_gain",
                     "capital_loss", "hours_per_week")

for (col in continuous_cols) {
  # Get real and synthetic values
  real_vals <- adult[[col]]
  synth_vals <- synthetic_data[[col]]

  # Plot histograms
  hist(real_vals, col = rgb(0.2, 0.4, 0.6, 0.5),
       main = col, xlab = col,
       breaks = 30, freq = FALSE)
  hist(synth_vals, col = rgb(0.8, 0.2, 0.2, 0.5),
       add = TRUE, breaks = 30, freq = FALSE)
  legend("topright", c("Real", "Synthetic"),
         fill = c(rgb(0.2, 0.4, 0.6, 0.5), rgb(0.8, 0.2, 0.2, 0.5)))
}
```

### Categorical Variable Comparison

```{r categorical-comparison, fig.width=12, fig.height=6}
par(mfrow = c(2, 4))

categorical_cols <- c("workclass", "education", "marital_status", "occupation",
                      "relationship", "race", "sex", "income")

for (col in categorical_cols) {
  real_props <- prop.table(table(adult[[col]]))
  synth_props <- prop.table(table(synthetic_data[[col]]))

  # Align categories
  all_cats <- union(names(real_props), names(synth_props))
  real_aligned <- sapply(all_cats, function(x) ifelse(x %in% names(real_props), real_props[x], 0))
  synth_aligned <- sapply(all_cats, function(x) ifelse(x %in% names(synth_props), synth_props[x], 0))

  barplot(rbind(real_aligned, synth_aligned), beside = TRUE,
          col = c(rgb(0.2, 0.4, 0.6, 0.7), rgb(0.8, 0.2, 0.2, 0.7)),
          main = col, las = 2, cex.names = 0.7)
}
legend("topright", c("Real", "Synthetic"),
       fill = c(rgb(0.2, 0.4, 0.6, 0.7), rgb(0.8, 0.2, 0.2, 0.7)))
```

### Statistical Comparison

```{r statistical-comparison}
# Compare summary statistics for continuous columns
cat("\n=== Continuous Variables: Mean Comparison ===\n")
for (col in continuous_cols) {
  real_mean <- mean(adult[[col]], na.rm = TRUE)
  synth_mean <- mean(synthetic_data[[col]], na.rm = TRUE)
  diff_pct <- abs(real_mean - synth_mean) / real_mean * 100
  cat(sprintf("%s: Real=%.2f, Synthetic=%.2f (%.1f%% diff)\n",
              col, real_mean, synth_mean, diff_pct))
}

cat("\n=== Continuous Variables: Std Dev Comparison ===\n")
for (col in continuous_cols) {
  real_sd <- sd(adult[[col]], na.rm = TRUE)
  synth_sd <- sd(synthetic_data[[col]], na.rm = TRUE)
  diff_pct <- abs(real_sd - synth_sd) / real_sd * 100
  cat(sprintf("%s: Real=%.2f, Synthetic=%.2f (%.1f%% diff)\n",
              col, real_sd, synth_sd, diff_pct))
}

# Compare correlation structure
cat("\n=== Correlation Matrix Comparison (Continuous) ===\n")
real_cor <- cor(adult[, continuous_cols], use = "complete.obs")
synth_cor <- cor(synthetic_data[, continuous_cols], use = "complete.obs")
cor_diff <- abs(real_cor - synth_cor)
cat("Max absolute correlation difference:", max(cor_diff), "\n")
cat("Mean absolute correlation difference:", mean(cor_diff), "\n")
```

## Saving and Loading the Model

Save the trained model for later use:

```{r save-model}
# Save the trained GAN
save_gan(trained_gan, "adult_sota_gan")

# Later, load it back
loaded_gan <- load_gan("adult_sota_gan")

# Generate more samples with the loaded model
more_synthetic <- sample_synthetic_data(loaded_gan, transformer, n = 500)
```

## Advanced: Custom Generator with Self-Attention

For datasets where features have complex interdependencies (e.g., age correlates with
income, education correlates with occupation), self-attention can capture these
relationships:

```{r custom-generator}
# Create a custom generator with self-attention
# Attention is applied after specified layers to capture feature relationships
custom_gen <- TabularGenerator(
  noise_dim = 128,
  output_info = transformer$output_info,
  hidden_units = list(256, 256, 256, 256),  # 4 layers
  tau = 0.1,                      # Low temperature for discrete categoricals
  normalization = "batch",        # Batch norm (most stable)
  activation = "leaky_relu",      # LeakyReLU
  init_method = "xavier_uniform",
  residual = TRUE,
  attention = c(2, 4),            # Attention after layers 2 and 4
  attention_heads = 8,            # More heads = finer-grained attention
  attention_dropout = 0.1
)

# Move to device
custom_gen <- custom_gen$to(device = device)

# Use in training with the custom generator
trained_custom <- gan_trainer(
  data = train_data,
  noise_dim = 128,
  generator = custom_gen,
  value_function = "wgan-gp",
  gp_lambda = 10,
  batch_size = 500,
  epochs = 100,
  base_lr = 1e-4,
  ttur_factor = 2,
  pac = 10,
  lr_schedule = "cosine",
  track_loss = TRUE,
  device = device,
  seed = 42
)

# Sample from the attention-enhanced generator
synthetic_attention <- sample_synthetic_data(trained_custom, transformer, n = 1000)
```

```{r plot-losses}
# Plot training losses (requires track_loss = TRUE during training)
if (!is.null(trained_gan$losses)) {
  plot_losses(trained_custom, smooth = 0.9)
}
```
## Evaluating Synthetic Data Quality

### Visual Comparison

```{r visual-comparison, fig.width=10, fig.height=8}
# Compare distributions of continuous variables
par(mfrow = c(2, 3))

continuous_cols <- c("age", "education_num", "capital_gain",
                     "capital_loss", "hours_per_week")

for (col in continuous_cols) {
  # Get real and synthetic values
  real_vals <- adult[[col]]
  synth_vals <- synthetic_attention[[col]]

  # Plot histograms
  hist(real_vals, col = rgb(0.2, 0.4, 0.6, 0.5),
       main = col, xlab = col,
       breaks = 30, freq = FALSE)
  hist(synth_vals, col = rgb(0.8, 0.2, 0.2, 0.5),
       add = TRUE, breaks = 30, freq = FALSE)
  legend("topright", c("Real", "Synthetic"),
         fill = c(rgb(0.2, 0.4, 0.6, 0.5), rgb(0.8, 0.2, 0.2, 0.5)))
}
```

### Categorical Variable Comparison

```{r categorical-comparison, fig.width=12, fig.height=6}
par(mfrow = c(2, 4))

categorical_cols <- c("workclass", "education", "marital_status", "occupation",
                      "relationship", "race", "sex", "income")

for (col in categorical_cols) {
  real_props <- prop.table(table(adult[[col]]))
  synth_props <- prop.table(table(synthetic_attention[[col]]))

  # Align categories
  all_cats <- union(names(real_props), names(synth_props))
  real_aligned <- sapply(all_cats, function(x) ifelse(x %in% names(real_props), real_props[x], 0))
  synth_aligned <- sapply(all_cats, function(x) ifelse(x %in% names(synth_props), synth_props[x], 0))

  barplot(rbind(real_aligned, synth_aligned), beside = TRUE,
          col = c(rgb(0.2, 0.4, 0.6, 0.7), rgb(0.8, 0.2, 0.2, 0.7)),
          main = col, las = 2, cex.names = 0.7)
}
legend("topright", c("Real", "Synthetic"),
       fill = c(rgb(0.2, 0.4, 0.6, 0.7), rgb(0.8, 0.2, 0.2, 0.7)))
```

## Advanced: Progressive Training

For very deep networks, progressive training can help:
```{r progressive-training}
# Create generator with many layers
deep_gen <- TabularGenerator(
  noise_dim = 128,
  output_info = transformer$output_info,
  hidden_units = list(128, 256, 256, 256, 128),
  attention = TRUE,
  attention_heads = 4
)$to(device = device)

# Start with only 2 blocks active
deep_gen$set_active_blocks(2)

# Train for some epochs
# ... training code ...

# Gradually add more blocks
deep_gen$set_active_blocks(3)
# ... more training ...

deep_gen$set_active_blocks(4)
# ... more training ...

# Finally use all blocks
deep_gen$set_active_blocks(5)
# ... final training ...
```

## Tips for Best Results

### 1. Data Preprocessing

- **Use mode-specific normalization** for columns with multi-modal distributions (like income,
  capital gains with spikes at 0)
- **Increase `n_modes`** (15-20) for complex distributions - the GMM needs enough components
  to capture the data structure
- **Handle outliers**: Consider clipping extreme values before training

### 2. Architecture

- **Deeper networks** (4 layers) have more capacity to learn complex distributions
- **Enable residual connections** (`generator_residual = TRUE`) for stable gradient flow
- **Use LeakyReLU** instead of ReLU - prevents "dying neurons"
- **Try self-attention** for datasets where features have complex dependencies

### 3. Training Dynamics

- **Use WGAN-GP** for stable training - it rarely suffers from mode collapse
- **Enable TTUR** (`ttur_factor = 2-4`) - training the discriminator faster helps the
  generator learn better
- **Lower learning rate** (1e-4 or 5e-5) for more stable convergence
- **More epochs** (500+) - GANs often need longer training than you expect
- **Higher patience** (50+) for early stopping - GAN losses can be noisy

### 4. Mode Collapse Prevention

- **Enable PacGAN** (`pac = 8-10`) - the discriminator sees multiple samples together,
  helping it detect lack of diversity
- **Monitor diversity metrics** during training if possible
- If you see repeated identical samples, increase `pac` or training time

### 5. Batch Size

- **Larger batches** (500-1000) provide more stable gradient estimates
- Ensure `batch_size` is divisible by `pac`
- If memory-limited, reduce batch size but increase epochs proportionally

### 6. Gumbel-Softmax Temperature

- **Lower tau (0.1)** produces more discrete categorical outputs (better for final samples)
- **Higher tau (0.5-1.0)** provides smoother gradients (can help early training)
- Consider temperature annealing: start high, reduce during training

### 7. Post-Processing (if needed)

After generating synthetic data, you may want to:

```{r post-processing, eval=FALSE}
# Post-process synthetic data to ensure valid ranges

# Clip continuous values to observed ranges in real data
synthetic_data$age <- pmax(17, pmin(90, round(synthetic_data$age)))
synthetic_data$education_num <- pmax(1, pmin(16, round(synthetic_data$education_num)))
synthetic_data$hours_per_week <- pmax(1, pmin(99, round(synthetic_data$hours_per_week)))

# Capital gain/loss are non-negative and often 0
synthetic_data$capital_gain <- pmax(0, synthetic_data$capital_gain)
synthetic_data$capital_loss <- pmax(0, synthetic_data$capital_loss)

# Optional: Round capital values to integers (as in original data)
synthetic_data$capital_gain <- round(synthetic_data$capital_gain)
synthetic_data$capital_loss <- round(synthetic_data$capital_loss)
```

### 8. Hyperparameter Search

If results aren't satisfactory, try varying:
- `noise_dim`: 64, 128, 256
- `generator_hidden_units`: `list(128, 128)` to `list(512, 512, 512, 512)`
- `base_lr`: 5e-5 to 5e-4
- `gumbel_tau`: 0.1 to 0.5
- `n_modes`: 5 to 20

## Session Info

```{r session-info}
sessionInfo()
```
