---
title: "Training GANs with Differential Privacy"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Training GANs with Differential Privacy}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

This vignette demonstrates how to train GANs with **differential privacy** (DP) guarantees
using the `dp_gan_trainer()` function. Differential privacy provides mathematically
rigorous privacy guarantees, ensuring that the trained model doesn't reveal too much
information about any individual in the training data.

## Overview

Differential privacy in machine learning is achieved through **DP-SGD** (Differentially
Private Stochastic Gradient Descent), introduced by Abadi et al. (2016). The key
mechanisms are:

1. **Poisson Subsampling**: Each training example is included in a batch independently
   with probability q, providing privacy amplification
2. **Per-Sample Gradient Clipping**: Bounds the influence of any single training example
3. **Gaussian Noise Addition**: Calibrated noise is added to gradients to mask individual
   contributions
4. **Privacy Accounting**: Tracks cumulative privacy loss using Rényi Differential Privacy

The `dp_gan_trainer()` function implements these mechanisms using the **OpenDP** library
for cryptographically secure random number generation.

## Setup

```{r setup}
library(RGAN)
library(torch)

# Check for OpenDP availability
if (!requireNamespace("opendp", quietly = TRUE)) {
  stop("This vignette requires the 'opendp' package. Install it with: install.packages('opendp', repos = 'https://opendp.r-universe.dev/')")
}

# Set seed for reproducibility
set.seed(42)

# Check for GPU availability
device <- if (cuda_is_available()) {
  "cuda"
} else if (torch::backends_mps_is_available()) {
 "mps"
} else {
  "cpu"
}

cat("Using device:", device, "\n")
```

## Understanding Privacy Parameters

Before training, it's important to understand the key privacy parameters:

- **epsilon (ε)**: The privacy budget. Lower values = stronger privacy guarantees.
  Common values range from 0.1 (very private) to 10 (relaxed privacy).
- **delta (δ)**: The probability of privacy failure. Typically set to 1/n or smaller,
  where n is the dataset size. Common value: 1e-5.
- **max_grad_norm**: Maximum norm for gradient clipping. Bounds sensitivity of individual
  gradients. Typical values: 0.1 to 10.
- **noise_multiplier**: Ratio of noise standard deviation to sensitivity. Higher values
  = more privacy but less utility. Can be auto-calibrated from target epsilon.

The (ε, δ)-differential privacy guarantee means: for any two datasets differing in one
record, the probability of any output is at most e^ε times higher, plus δ.

## Loading and Preparing Data

We'll use a simple toy dataset to demonstrate DP training:

```{r load-data}
# Generate a toy dataset with multiple modes
n_samples <- 2000

# Create a mixture of Gaussians
set.seed(42)
data <- rbind(
  matrix(rnorm(n_samples, mean = c(-2, -2), sd = 0.5), ncol = 2, byrow = TRUE),
  matrix(rnorm(n_samples, mean = c(2, 2), sd = 0.5), ncol = 2, byrow = TRUE),
  matrix(rnorm(n_samples, mean = c(-2, 2), sd = 0.5), ncol = 2, byrow = TRUE),
  matrix(rnorm(n_samples, mean = c(2, -2), sd = 0.5), ncol = 2, byrow = TRUE)
)

colnames(data) <- c("x", "y")

cat("Dataset size:", nrow(data), "samples\n")

# Visualize the data
plot(data, col = rgb(0, 0, 0, 0.3), pch = 19, main = "Training Data",
     xlab = "x", ylab = "y", asp = 1)
```

## Data Transformation

```{r transform-data}
# Create and fit the data transformer
transformer <- data_transformer$new()
transformer$fit(data)

# Transform the data
transformed_data <- transformer$transform(data)

cat("Transformed data shape:", dim(transformed_data), "\n")
```

## Training with Differential Privacy

### Basic DP Training

Train a GAN with a target privacy budget of ε = 1:

```{r dp-training-basic}
# Train with differential privacy
dp_gan <- dp_gan_trainer(
  data = transformed_data,

  # === Privacy Parameters ===
  target_epsilon = 1.0,       # Privacy budget
  target_delta = 1e-5,        # Privacy failure probability
  max_grad_norm = 1.0,        # Gradient clipping norm
  # noise_multiplier is auto-calibrated to achieve target_epsilon

  # === Architecture ===
  noise_dim = 32,

  # === Training Parameters ===
  batch_size = 100,
  epochs = 50,
  base_lr = 0.001,

  # === Monitoring ===
  track_loss = TRUE,
  verbose = TRUE,

  # === Device ===
  device = device,
  seed = 42
)

# Check final privacy guarantee
cat("\n=== Privacy Accounting ===\n")
cat(sprintf("Final epsilon: %.4f\n", dp_gan$privacy$final_epsilon))
cat(sprintf("Delta: %.2e\n", dp_gan$privacy$delta))
cat(sprintf("Noise multiplier: %.4f\n", dp_gan$privacy$noise_multiplier))
cat(sprintf("Max gradient norm: %.4f\n", dp_gan$privacy$max_grad_norm))
cat(sprintf("Sampling rate: %.4f\n", dp_gan$privacy$sampling_rate))
cat(sprintf("Total training steps: %d\n", dp_gan$privacy$total_steps))
```

### Generating Synthetic Data

```{r generate-dp-synthetic}
# Generate synthetic data from the DP-trained GAN
dp_synthetic <- sample_synthetic_data(dp_gan, transformer, n = 2000)

cat("Generated", nrow(dp_synthetic), "synthetic samples\n")
```

### Visualizing Results

```{r visualize-dp, fig.width=10, fig.height=5}
par(mfrow = c(1, 2))

# Real data
plot(data, col = rgb(0.2, 0.4, 0.6, 0.5), pch = 19,
     main = "Real Data", xlab = "x", ylab = "y", asp = 1,
     xlim = c(-5, 5), ylim = c(-5, 5))

# DP synthetic data
plot(dp_synthetic, col = rgb(0.8, 0.2, 0.2, 0.5), pch = 19,
     main = sprintf("DP Synthetic (ε = %.2f)", dp_gan$privacy$final_epsilon),
     xlab = "x", ylab = "y", asp = 1,
     xlim = c(-5, 5), ylim = c(-5, 5))
```

## Comparing Different Privacy Levels

Let's train models with different privacy budgets to see the privacy-utility tradeoff:

```{r compare-privacy-levels}
# Train models with different epsilon values
epsilon_values <- c(0.5, 1.0, 5.0, 10.0)
dp_models <- list()

for (eps in epsilon_values) {
  cat(sprintf("\n--- Training with epsilon = %.1f ---\n", eps))

  dp_models[[as.character(eps)]] <- dp_gan_trainer(
    data = transformed_data,
    target_epsilon = eps,
    target_delta = 1e-5,
    max_grad_norm = 1.0,
    noise_dim = 32,
    batch_size = 100,
    epochs = 50,
    base_lr = 0.001,
    verbose = FALSE,
    device = device,
    seed = 42
  )

  cat(sprintf("Final epsilon: %.4f\n", dp_models[[as.character(eps)]]$privacy$final_epsilon))
}
```

### Visual Comparison

```{r compare-visual, fig.width=12, fig.height=10}
par(mfrow = c(2, 3))

# Real data
plot(data, col = rgb(0.2, 0.4, 0.6, 0.5), pch = 19,
     main = "Real Data", xlab = "x", ylab = "y", asp = 1,
     xlim = c(-5, 5), ylim = c(-5, 5))

# Synthetic data for each epsilon
for (eps in epsilon_values) {
  synth <- sample_synthetic_data(dp_models[[as.character(eps)]], transformer, n = 2000)
  actual_eps <- dp_models[[as.character(eps)]]$privacy$final_epsilon

  plot(synth, col = rgb(0.8, 0.2, 0.2, 0.5), pch = 19,
       main = sprintf("ε = %.2f", actual_eps),
       xlab = "x", ylab = "y", asp = 1,
       xlim = c(-5, 5), ylim = c(-5, 5))
}

# Legend
plot.new()
legend("center",
       legend = c("Real Data", "DP Synthetic"),
       col = c(rgb(0.2, 0.4, 0.6), rgb(0.8, 0.2, 0.2)),
       pch = 19, bty = "n", cex = 1.5)
```

### Quantitative Comparison

```{r compare-metrics}
cat("\n=== Privacy-Utility Tradeoff ===\n")
cat(sprintf("%-12s %12s %12s %12s %12s\n",
            "Epsilon", "Mean X Err", "Mean Y Err", "Std X Err", "Std Y Err"))
cat(paste(rep("-", 62), collapse = ""), "\n")

real_mean_x <- mean(data[, 1])
real_mean_y <- mean(data[, 2])
real_sd_x <- sd(data[, 1])
real_sd_y <- sd(data[, 2])

for (eps in epsilon_values) {
  synth <- sample_synthetic_data(dp_models[[as.character(eps)]], transformer, n = 2000)
  actual_eps <- dp_models[[as.character(eps)]]$privacy$final_epsilon

  mean_x_err <- abs(mean(synth[, 1]) - real_mean_x)
  mean_y_err <- abs(mean(synth[, 2]) - real_mean_y)
  sd_x_err <- abs(sd(synth[, 1]) - real_sd_x)
  sd_y_err <- abs(sd(synth[, 2]) - real_sd_y)

  cat(sprintf("%-12.2f %12.4f %12.4f %12.4f %12.4f\n",
              actual_eps, mean_x_err, mean_y_err, sd_x_err, sd_y_err))
}
```

## Combining DP Training with Post-GAN Boosting

For the best of both worlds, you can combine DP training with post-GAN boosting.
However, there's an important consideration: **the discriminator must output scores
bounded between 0 and 1** (using sigmoid activation) for the privacy analysis of
DP post-GAN boosting to be correct.

### Training a DP-GAN with Checkpoints

First, we need to modify our approach. The `dp_gan_trainer` already uses sigmoid
output for the discriminator by default. To enable post-GAN boosting, we need to
manually train with checkpoints.

```{r dp-pgb-training}
# For DP + Post-GAN Boosting, train with checkpoints
# The discriminator MUST use sigmoid output for valid privacy analysis

# Create networks with sigmoid discriminator
g_net <- Generator(noise_dim = 32, data_dim = ncol(transformed_data), dropout_rate = 0.5)
d_net <- Discriminator(data_dim = ncol(transformed_data), dropout_rate = 0.5, sigmoid = TRUE)

# Train with standard gan_trainer but using the value function that's compatible
# with bounded discriminator scores
dp_pgb_gan <- gan_trainer(
  data = transformed_data,

  # === Networks ===
  generator = g_net$to(device = device),
  discriminator = d_net$to(device = device),

  # === Value Function ===
  # Use "original" value function which expects sigmoid discriminator
  value_function = "original",

  # === Architecture ===
  noise_dim = 32,

  # === Training Parameters ===
  batch_size = 100,
  epochs = 100,
  base_lr = 0.0001,

  # === Checkpoints for Post-GAN Boosting ===
  checkpoint_epochs = 10,  # Save every 10 epochs

  # === Monitoring ===
  track_loss = TRUE,

  # === Device ===
  device = device,
  seed = 42
)

cat("Checkpoints captured at epochs:", dp_pgb_gan$checkpoints$epochs, "\n")
```

### Applying DP Post-GAN Boosting

Now apply post-GAN boosting with differential privacy:

```{r apply-dp-pgb}
# Apply post-GAN boosting with differential privacy
# The dp=TRUE flag uses the exponential mechanism for private discriminator selection
boosted_result <- apply_post_gan_boosting(
  trained_gan = dp_pgb_gan,
  real_data = transformed_data,
  transformer = transformer,
  n_candidates = 1000,
  steps = 200,
  dp = TRUE,              # Enable differential privacy
  MW_epsilon = 1.0,       # Privacy budget for multiplicative weights
  seed = 123
)

cat("DP-Boosted samples:", boosted_result$n_unique, "\n")
```

### Comparing Standard vs DP-Boosted

```{r compare-dp-pgb, fig.width=12, fig.height=4}
# Generate standard synthetic data
standard_synthetic <- sample_synthetic_data(dp_pgb_gan, transformer, n = 2000)

# Get DP-boosted synthetic data
boosted_synthetic <- boosted_result$samples

par(mfrow = c(1, 3))

# Real data
plot(data, col = rgb(0.2, 0.4, 0.6, 0.5), pch = 19,
     main = "Real Data", xlab = "x", ylab = "y", asp = 1,
     xlim = c(-5, 5), ylim = c(-5, 5))

# Standard synthetic
plot(standard_synthetic, col = rgb(0.8, 0.4, 0.1, 0.5), pch = 19,
     main = "Standard Synthetic", xlab = "x", ylab = "y", asp = 1,
     xlim = c(-5, 5), ylim = c(-5, 5))

# DP-boosted synthetic
plot(boosted_synthetic[, 1:2], col = rgb(0.2, 0.7, 0.3, 0.5), pch = 19,
     main = sprintf("DP-Boosted (ε = %.1f)", 1.0),
     xlab = "x", ylab = "y", asp = 1,
     xlim = c(-5, 5), ylim = c(-5, 5))
```

## Understanding the Privacy Budget

### How Privacy Accumulates

In DP-SGD, privacy loss accumulates with each training step. The `dp_gan_trainer`
uses **Rényi Differential Privacy (RDP)** for tight privacy accounting:

```{r privacy-accumulation}
# Create an accountant to understand privacy accumulation
accountant <- dp_accountant_poisson$new(
  sampling_rate = 0.05,      # batch_size / n_samples
  noise_multiplier = 1.0,
  target_delta = 1e-5
)

# Track epsilon over training steps
steps <- seq(0, 1000, by = 50)
epsilons <- sapply(steps, function(s) {
  accountant$steps <- s
  accountant$get_epsilon()
})

plot(steps, epsilons, type = "l", lwd = 2, col = "blue",
     main = "Privacy Budget Consumption",
     xlab = "Training Steps", ylab = "Epsilon",
     ylim = c(0, max(epsilons) * 1.1))
abline(h = 1.0, lty = 2, col = "red")
text(500, 1.1, "Target ε = 1.0", col = "red")
```

### Calibrating Noise for Target Privacy

The `calibrate_noise_multiplier()` function finds the appropriate noise level
for a given privacy budget:

```{r calibrate-noise}
# Find noise multiplier for different privacy targets
targets <- c(0.5, 1.0, 2.0, 5.0, 10.0)

cat("\n=== Noise Calibration for 1000 Steps ===\n")
cat(sprintf("%-12s %18s\n", "Target ε", "Noise Multiplier"))
cat(paste(rep("-", 32), collapse = ""), "\n")

for (target in targets) {
  noise <- calibrate_noise_multiplier(
    target_epsilon = target,
    target_delta = 1e-5,
    sampling_rate = 0.05,
    total_steps = 1000
  )
  cat(sprintf("%-12.1f %18.4f\n", target, noise))
}
```

## Best Practices for DP-GAN Training

### 1. Choosing Privacy Parameters

- **Target epsilon**: Start with ε = 1-10 for initial experiments. ε < 1 provides
  strong privacy but may significantly impact utility.
- **Delta**: Use δ ≤ 1/n where n is dataset size. Default 1e-5 is typically safe.
- **Gradient clipping**: Start with `max_grad_norm = 1.0`. Adjust based on typical
  gradient magnitudes in your model.

### 2. Training Configuration

- **Batch size**: Larger batches provide better privacy amplification (more
  subsampling). Recommended: 100-500.
- **Epochs**: DP training often needs fewer epochs since each step consumes privacy
  budget. Monitor epsilon consumption.
- **Learning rate**: May need to be higher than non-DP training to compensate for
  noise. Start with 0.001-0.01.

### 3. Monitoring Training

```{r monitoring-tips}
# Always enable verbose mode to track privacy consumption
# Use track_loss = TRUE to monitor convergence

# Example: check if training converged before budget exhausted
if (!is.null(dp_gan$losses)) {
  # Plot loss curves
  par(mfrow = c(1, 2))

  plot(dp_gan$losses$d_loss, type = "l", col = "blue",
       main = "Discriminator Loss", xlab = "Step", ylab = "Loss")

  plot(dp_gan$losses$g_loss, type = "l", col = "red",
       main = "Generator Loss", xlab = "Step", ylab = "Loss")
}
```

### 4. Privacy-Utility Tradeoff

- Higher privacy (lower ε) = more noise = lower utility
- More training steps = higher ε = potentially better utility but weaker privacy
- Larger datasets benefit more from privacy amplification by subsampling

### 5. When to Use DP-GANs

DP-GANs are appropriate when:
- Training data contains sensitive information (medical records, financial data)
- Regulations require privacy guarantees (GDPR, HIPAA)
- You need to publish synthetic data or models trained on private data
- You want formal, mathematical privacy guarantees

## Limitations and Considerations

1. **Utility degradation**: DP training typically produces lower-quality synthetic
   data compared to non-private training. The tradeoff depends on privacy budget
   and dataset size.

2. **Computational cost**: Per-sample gradient computation is slower than batch
   gradients. Consider using smaller models or fewer epochs.

3. **Privacy accounting**: The privacy guarantees assume correct implementation
   of all mechanisms (subsampling, clipping, noise). RGAN uses OpenDP for
   cryptographically secure operations.

4. **Composition**: If you perform multiple analyses on the same data, privacy
   budgets compose. Plan your total budget accordingly.

## Session Info

```{r session-info}
sessionInfo()
```

## References

Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., & Zhang, L. (2016).
Deep learning with differential privacy. In *Proceedings of the 2016 ACM SIGSAC conference
on computer and communications security* (pp. 308-318).

Mironov, I. (2017). Rényi differential privacy. In *2017 IEEE 30th computer security
foundations symposium (CSF)* (pp. 263-275).

Neunhoeffer, M., Wu, Z. S., & Dwork, C. (2021). Private Post-GAN Boosting.
*International Conference on Learning Representations (ICLR)*.
