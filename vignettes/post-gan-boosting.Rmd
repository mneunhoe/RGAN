---
title: "Improving Synthetic Data Quality with Post-GAN Boosting"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Improving Synthetic Data Quality with Post-GAN Boosting}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

This vignette demonstrates how to use **Post-GAN Boosting** (Neunhoeffer et al., ICLR 2021)
to improve the quality of synthetic data generated by GANs. Post-GAN boosting uses an
ensemble of discriminators captured during training to select high-quality samples from
a pool of candidates.

## Overview

Traditional GAN training returns only the final generator, but intermediate discriminators
contain valuable information about data quality. Post-GAN boosting leverages this by:

1. **Capturing checkpoints** during training (both generators and discriminators)
2. **Generating candidate samples** from all checkpointed generators
3. **Scoring candidates** using all checkpointed discriminators
4. **Selecting high-quality samples** using the multiplicative weights algorithm

This approach can significantly improve sample quality, especially for:
- Multi-modal distributions
- Datasets with rare categories
- Cases where the final generator hasn't fully converged

## Setup

```{r setup}
library(RGAN)
library(torch)

# Set seed for reproducibility
set.seed(42)

# Check for GPU availability
device <- if (cuda_is_available()) {
  "cuda"
} else if (torch::backends_mps_is_available()) {
 "mps"
} else {
  "cpu"
}

cat("Using device:", device, "\n")
```

## Loading and Preparing the Adult Dataset

```{r load-data}
# Download the Adult dataset from UCI Machine Learning Repository
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"

col_names <- c(
  "age", "workclass", "fnlwgt", "education", "education_num",
  "marital_status", "occupation", "relationship", "race", "sex",
  "capital_gain", "capital_loss", "hours_per_week", "native_country", "income"
)

adult <- read.csv(
  url,
  header = FALSE,
  col.names = col_names,
  na.strings = " ?",
  strip.white = TRUE,
  stringsAsFactors = FALSE
)

# Remove rows with missing values
adult <- na.omit(adult)

# Select columns for synthesis
adult <- adult[, c(
  "age", "workclass", "education", "education_num", "marital_status",
  "occupation", "relationship", "race", "sex",
  "capital_gain", "capital_loss", "hours_per_week", "income"
)]

cat("Dataset size:", nrow(adult), "rows,", ncol(adult), "columns\n")
```

## Data Transformation

```{r transform-data}
# Create and fit the data transformer
transformer <- data_transformer$new()

discrete_cols <- c(
  "workclass", "education", "marital_status",
  "occupation", "relationship", "race", "sex", "income"
)

# Fit with mode-specific normalization
transformer$fit(
  adult,
  discrete_columns = discrete_cols,
  mode_specific = TRUE,
  n_modes = 15
)

# Transform the data
transformed_data <- transformer$transform(adult)

cat("Transformed data shape:", dim(transformed_data), "\n")
```

## Training with Checkpoints

The key to post-GAN boosting is capturing intermediate model states during training.
Use the `checkpoint_epochs` parameter to specify how often to save checkpoints:

```{r train-with-checkpoints}
# Train the GAN with checkpoint capture
trained_gan <- gan_trainer(
  data = transformed_data,

  # === Noise Configuration ===
  noise_dim = 128,
  noise_distribution = "normal",

  # === Value Function ===
  value_function = "wgan-gp",
  gp_lambda = 10,

  # === Generator Architecture ===
  output_info = transformer$output_info,
  gumbel_tau = 0.1,
  generator_hidden_units = list(256, 256, 256, 256),
  generator_normalization = "batch",
  generator_activation = "leaky_relu",
  generator_init = "xavier_uniform",
  generator_residual = TRUE,

  # === Training Parameters ===
  batch_size = 500,
  epochs = 200,
  base_lr = 1e-4,
  ttur_factor = 2,

  # === PacGAN ===
  pac = 10,

  # === Learning Rate Schedule ===
  lr_schedule = "cosine",

  # === CHECKPOINT CONFIGURATION ===
  checkpoint_epochs = 20,  # Save every 20 epochs (will capture 10 checkpoints)

  # === Monitoring ===
  track_loss = TRUE,

  # === Device ===
  device = device,
  seed = 42
)

# Check the captured checkpoints
cat("Number of checkpoints captured:", length(trained_gan$checkpoints$epochs), "\n")
cat("Checkpoint epochs:", trained_gan$checkpoints$epochs, "\n")
```

### Understanding Checkpoint Storage

Checkpoints are stored in the `trained_gan$checkpoints` list:

- `epochs`: Vector of epoch numbers when checkpoints were saved
- `generators`: List of generator state dictionaries
- `discriminators`: List of discriminator state dictionaries
- `on_disk`: Whether checkpoints are stored on disk (FALSE for in-memory)

For large training runs with many checkpoints, you can save to disk:

```{r disk-checkpoints, eval=FALSE}
# Save checkpoints to disk (for large training runs)
trained_gan_disk <- gan_trainer(
  data = transformed_data,
  # ... other parameters ...
  checkpoint_epochs = 10,
  checkpoint_path = "checkpoints/adult_gan",  # Directory for checkpoint files
  device = device
)
```

## Generating Standard Synthetic Data

First, let's generate synthetic data using only the final generator (standard approach):

```{r standard-synthetic}
# Generate synthetic data from the final generator
n_synthetic <- 5000

standard_synthetic <- sample_synthetic_data(
  trained_gan,
  transformer,
  n = n_synthetic
)

cat("Standard synthetic data shape:", dim(standard_synthetic), "\n")
```

## Applying Post-GAN Boosting

Now let's apply post-GAN boosting to select high-quality samples:

```{r apply-boosting}
# Apply post-GAN boosting
# This generates candidates from ALL checkpointed generators and selects the best

boosted_result <- apply_post_gan_boosting(
  trained_gan = trained_gan,
  real_data = transformed_data,
  transformer = transformer,
  n_candidates = 1000,   # Generate 1000 candidates per generator checkpoint
  steps = 200,           # Number of boosting iterations
  dp = FALSE,            # No differential privacy (set TRUE for privacy guarantees)
  seed = 123
)

cat("Boosted samples:", boosted_result$n_unique, "\n")

# Extract the boosted synthetic data
boosted_synthetic <- boosted_result$samples
```

### Post-GAN Boosting Parameters

- `n_candidates`: Number of synthetic samples to generate from each generator checkpoint.
  More candidates = better selection pool but slower.
- `steps`: Number of multiplicative weights iterations. More steps = better convergence
  but slower.
- `dp`: Enable differential privacy via the exponential mechanism. When TRUE, set
  `MW_epsilon` for the privacy budget.
- `weighted_average`: Use time-weighted averaging of the sample distribution.
- `averaging_window`: Number of final steps to average over (default: all steps).

## Comparing Standard vs Boosted Synthetic Data

### Summary Statistics

```{r compare-stats}
continuous_cols <- c("age", "education_num", "capital_gain",
                     "capital_loss", "hours_per_week")

cat("\n=== Mean Comparison ===\n")
cat(sprintf("%-15s %10s %10s %10s %10s\n",
            "Column", "Real", "Standard", "Boosted", "Winner"))
cat(paste(rep("-", 60), collapse = ""), "\n")

for (col in continuous_cols) {
  real_mean <- mean(adult[[col]], na.rm = TRUE)
  standard_mean <- mean(standard_synthetic[[col]], na.rm = TRUE)
  boosted_mean <- mean(boosted_synthetic[[col]], na.rm = TRUE)

  standard_diff <- abs(real_mean - standard_mean)
  boosted_diff <- abs(real_mean - boosted_mean)

  winner <- ifelse(boosted_diff < standard_diff, "Boosted", "Standard")

  cat(sprintf("%-15s %10.2f %10.2f %10.2f %10s\n",
              col, real_mean, standard_mean, boosted_mean, winner))
}

cat("\n=== Standard Deviation Comparison ===\n")
cat(sprintf("%-15s %10s %10s %10s %10s\n",
            "Column", "Real", "Standard", "Boosted", "Winner"))
cat(paste(rep("-", 60), collapse = ""), "\n")

for (col in continuous_cols) {
  real_sd <- sd(adult[[col]], na.rm = TRUE)
  standard_sd <- sd(standard_synthetic[[col]], na.rm = TRUE)
  boosted_sd <- sd(boosted_synthetic[[col]], na.rm = TRUE)

  standard_diff <- abs(real_sd - standard_sd)
  boosted_diff <- abs(real_sd - boosted_sd)

  winner <- ifelse(boosted_diff < standard_diff, "Boosted", "Standard")

  cat(sprintf("%-15s %10.2f %10.2f %10.2f %10s\n",
              col, real_sd, standard_sd, boosted_sd, winner))
}
```

### Visual Comparison: Continuous Variables

```{r compare-continuous, fig.width=12, fig.height=10}
par(mfrow = c(3, 5), mar = c(4, 4, 3, 1))

for (col in continuous_cols) {
  real_vals <- adult[[col]]
  standard_vals <- standard_synthetic[[col]]
  boosted_vals <- boosted_synthetic[[col]]

  # Determine common breaks for comparable histograms
  all_vals <- c(real_vals, standard_vals, boosted_vals)
  breaks <- seq(min(all_vals, na.rm = TRUE), max(all_vals, na.rm = TRUE), length.out = 31)

  # Real data
  hist(real_vals, col = rgb(0.2, 0.4, 0.6, 0.7), main = paste(col, "- Real"),
       xlab = col, breaks = breaks, freq = FALSE)

  # Standard synthetic
  hist(standard_vals, col = rgb(0.8, 0.2, 0.2, 0.7), main = paste(col, "- Standard"),
       xlab = col, breaks = breaks, freq = FALSE)

  # Boosted synthetic
  hist(boosted_vals, col = rgb(0.2, 0.7, 0.3, 0.7), main = paste(col, "- Boosted"),
       xlab = col, breaks = breaks, freq = FALSE)
}

# Add a legend row
plot.new()
legend("center", c("Real Data", "Standard Synthetic", "Boosted Synthetic"),
       fill = c(rgb(0.2, 0.4, 0.6, 0.7), rgb(0.8, 0.2, 0.2, 0.7), rgb(0.2, 0.7, 0.3, 0.7)),
       bty = "n", cex = 1.5)
```

### Visual Comparison: Categorical Variables

```{r compare-categorical, fig.width=14, fig.height=8}
categorical_cols <- c("workclass", "education", "marital_status", "occupation",
                      "relationship", "race", "sex", "income")

par(mfrow = c(2, 4), mar = c(8, 4, 3, 1))

for (col in categorical_cols) {
  real_props <- prop.table(table(adult[[col]]))
  standard_props <- prop.table(table(standard_synthetic[[col]]))
  boosted_props <- prop.table(table(boosted_synthetic[[col]]))

  # Align categories
  all_cats <- unique(c(names(real_props), names(standard_props), names(boosted_props)))

  real_aligned <- sapply(all_cats, function(x)
    ifelse(x %in% names(real_props), real_props[x], 0))
  standard_aligned <- sapply(all_cats, function(x)
    ifelse(x %in% names(standard_props), standard_props[x], 0))
  boosted_aligned <- sapply(all_cats, function(x)
    ifelse(x %in% names(boosted_props), boosted_props[x], 0))

  barplot(
    rbind(real_aligned, standard_aligned, boosted_aligned),
    beside = TRUE,
    col = c(rgb(0.2, 0.4, 0.6, 0.7), rgb(0.8, 0.2, 0.2, 0.7), rgb(0.2, 0.7, 0.3, 0.7)),
    main = col,
    las = 2,
    cex.names = 0.7
  )
}

# Add legend to last plot
legend("topright", c("Real", "Standard", "Boosted"),
       fill = c(rgb(0.2, 0.4, 0.6, 0.7), rgb(0.8, 0.2, 0.2, 0.7), rgb(0.2, 0.7, 0.3, 0.7)),
       bty = "n")
```

### Correlation Structure Comparison

```{r compare-correlations, fig.width=12, fig.height=4}
par(mfrow = c(1, 3))

# Compute correlation matrices
real_cor <- cor(adult[, continuous_cols], use = "complete.obs")
standard_cor <- cor(standard_synthetic[, continuous_cols], use = "complete.obs")
boosted_cor <- cor(boosted_synthetic[, continuous_cols], use = "complete.obs")

# Function to plot correlation matrix
plot_cor <- function(cor_mat, title) {
  n <- nrow(cor_mat)
  image(1:n, 1:n, cor_mat, col = hcl.colors(50, "RdBu"), zlim = c(-1, 1),
        axes = FALSE, main = title, xlab = "", ylab = "")
  axis(1, at = 1:n, labels = colnames(cor_mat), las = 2, cex.axis = 0.8)
  axis(2, at = 1:n, labels = colnames(cor_mat), las = 2, cex.axis = 0.8)
  # Add correlation values
  for (i in 1:n) {
    for (j in 1:n) {
      text(i, j, sprintf("%.2f", cor_mat[i, j]), cex = 0.7)
    }
  }
}

plot_cor(real_cor, "Real Data Correlations")
plot_cor(standard_cor, "Standard Synthetic Correlations")
plot_cor(boosted_cor, "Boosted Synthetic Correlations")
```

### Quantitative Quality Metrics

```{r quality-metrics}
# Compute quality metrics

# 1. Mean Absolute Error (MAE) for means
mae_means_standard <- mean(sapply(continuous_cols, function(col) {
  abs(mean(adult[[col]], na.rm = TRUE) - mean(standard_synthetic[[col]], na.rm = TRUE))
}))

mae_means_boosted <- mean(sapply(continuous_cols, function(col) {
  abs(mean(adult[[col]], na.rm = TRUE) - mean(boosted_synthetic[[col]], na.rm = TRUE))
}))

# 2. MAE for standard deviations
mae_sd_standard <- mean(sapply(continuous_cols, function(col) {
  abs(sd(adult[[col]], na.rm = TRUE) - sd(standard_synthetic[[col]], na.rm = TRUE))
}))

mae_sd_boosted <- mean(sapply(continuous_cols, function(col) {
  abs(sd(adult[[col]], na.rm = TRUE) - sd(boosted_synthetic[[col]], na.rm = TRUE))
}))

# 3. Correlation matrix difference (Frobenius norm)
cor_diff_standard <- sqrt(sum((real_cor - standard_cor)^2))
cor_diff_boosted <- sqrt(sum((real_cor - boosted_cor)^2))

# 4. Total Variation Distance for categorical columns
tvd_standard <- mean(sapply(categorical_cols, function(col) {
  real_p <- prop.table(table(factor(adult[[col]])))
  synth_p <- prop.table(table(factor(standard_synthetic[[col]], levels = names(real_p))))
  sum(abs(real_p - synth_p)) / 2
}))

tvd_boosted <- mean(sapply(categorical_cols, function(col) {
  real_p <- prop.table(table(factor(adult[[col]])))
  synth_p <- prop.table(table(factor(boosted_synthetic[[col]], levels = names(real_p))))
  sum(abs(real_p - synth_p)) / 2
}))

cat("\n=== Quality Metrics Summary ===\n")
cat(sprintf("%-40s %12s %12s\n", "Metric", "Standard", "Boosted"))
cat(paste(rep("-", 65), collapse = ""), "\n")
cat(sprintf("%-40s %12.4f %12.4f\n", "MAE of Means (lower is better)",
            mae_means_standard, mae_means_boosted))
cat(sprintf("%-40s %12.4f %12.4f\n", "MAE of Std Devs (lower is better)",
            mae_sd_standard, mae_sd_boosted))
cat(sprintf("%-40s %12.4f %12.4f\n", "Correlation Matrix Distance (lower is better)",
            cor_diff_standard, cor_diff_boosted))
cat(sprintf("%-40s %12.4f %12.4f\n", "Avg TVD for Categoricals (lower is better)",
            tvd_standard, tvd_boosted))

# Determine overall winner
standard_wins <- sum(c(mae_means_standard, mae_sd_standard, cor_diff_standard, tvd_standard) <
                     c(mae_means_boosted, mae_sd_boosted, cor_diff_boosted, tvd_boosted))
boosted_wins <- 4 - standard_wins

cat(paste(rep("-", 65), collapse = ""), "\n")
cat(sprintf("Overall: Standard wins %d/4 metrics, Boosted wins %d/4 metrics\n",
            standard_wins, boosted_wins))
```

## Using Post-GAN Boosting with Differential Privacy

For privacy-sensitive applications, post-GAN boosting supports differential privacy
through the exponential mechanism:

```{r dp-boosting, eval=FALSE}
# Apply post-GAN boosting with differential privacy
boosted_dp <- apply_post_gan_boosting(
  trained_gan = trained_gan,
  real_data = transformed_data,
  transformer = transformer,
  n_candidates = 1000,
  steps = 200,
  dp = TRUE,              # Enable differential privacy
  MW_epsilon = 1.0,       # Privacy budget (lower = more private, less accurate)
  seed = 123
)

cat("DP-Boosted samples:", boosted_dp$n_unique, "\n")
```

The privacy budget `MW_epsilon` is split across all boosting steps. Lower values
provide stronger privacy guarantees but may reduce sample quality.

## Low-Level API: Manual Discriminator Scoring

For advanced use cases, you can compute discriminator scores manually:

```{r manual-scoring}
# Generate candidate samples
n_candidates <- 500
candidates <- sample_synthetic_data(trained_gan, n_samples = n_candidates)

# Compute discriminator scores using all checkpoints
scores <- compute_discriminator_scores(
  trained_gan = trained_gan,
  generated_samples = candidates,
  real_data = transformed_data
)

cat("Score matrix shape:", dim(scores$d_score_fake), "\n")
cat("  Rows (discriminators):", nrow(scores$d_score_fake), "\n")
cat("  Columns (samples):", ncol(scores$d_score_fake), "\n")
cat("Checkpoint epochs:", scores$epochs, "\n")

# Examine score distributions
cat("\nMean discriminator score per checkpoint:\n")
for (i in seq_along(scores$epochs)) {
  cat(sprintf("  Epoch %3d: fake=%.4f, real=%.4f\n",
              scores$epochs[i],
              mean(scores$d_score_fake[i, ]),
              scores$d_score_real[i]))
}
```

You can then apply custom selection strategies or use the raw `post_gan_boosting()` function:

```{r manual-boosting}
# Apply boosting algorithm directly
result <- post_gan_boosting(
  d_score_fake = scores$d_score_fake,
  d_score_real = scores$d_score_real,
  B = candidates,
  real_N = nrow(transformed_data),
  steps = 100,
  N_generators = length(scores$epochs)
)

cat("Selected samples:", nrow(result$PGB_sample), "\n")
```

## Tips for Best Results with Post-GAN Boosting

### 1. Checkpoint Frequency

- **More checkpoints** (every 10-20 epochs) provide more discriminator diversity
- **Fewer checkpoints** (every 50 epochs) are faster but may miss good intermediate states
- For short training runs, use smaller intervals (e.g., `checkpoint_epochs = 5`)

### 2. Number of Candidates

- Start with `n_candidates = 1000` per generator
- Increase to 5000-10000 if you need more diverse/higher quality samples
- More candidates = better selection but longer computation

### 3. Boosting Steps

- Default `steps = 400` works well for most cases
- Reduce to 100-200 for faster results
- The algorithm converges relatively quickly

### 4. When to Use Post-GAN Boosting

Post-GAN boosting is most beneficial when:
- Training didn't fully converge (early stopping triggered)
- Data has rare categories or multi-modal distributions
- You need the highest quality samples possible
- You want to leverage all training information (not just final model)

### 5. Memory Considerations

- In-memory checkpoints (default) are fast but use RAM
- For long training runs (>500 epochs) with frequent checkpoints, use disk storage:
  ```r
  checkpoint_path = "path/to/checkpoints"
  ```

## Session Info

```{r session-info}
sessionInfo()
```

## References

Neunhoeffer, M., Wu, Z. S., & Dwork, C. (2021). Private Post-GAN Boosting.
*International Conference on Learning Representations (ICLR)*.
